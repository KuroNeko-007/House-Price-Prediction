{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Housing Price Predictor</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>1) Problem Statement</H2>\n",
    "\n",
    "The goal of this project is to build a machine learning model that can predict house prices based on various features such as LotArea, number of bedrooms, Neighborhood, year built, HouseStyle, OverallCond and more. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>2) Data Collection</H2>\n",
    "\n",
    "Dataset Source - https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview\n",
    "\n",
    "The data consists of 81 features and 1460 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Importing Packages</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd            # For data manipulation\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns          # For statistical visualizations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from optuna.integration import OptunaSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows',None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Import the CSV Data as Pandas DataFrame</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>3.1 Check Missing values</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()[train.isna().sum() > 0].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Check Duplicates</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>3.2 Handling Missing Values</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping high-NaN columns: Columns with a high percentage of missing values (e.g., more than 50%) are dropped from the dataset, as they provide little to no useful information.\n",
    "\n",
    "Imputation: For columns with fewer missing values, we use mean imputation for numerical features and mode imputation for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_nan_columns(df, threshold=0.5):\n",
    "    \"\"\"Drops columns with more than threshold% missing values.\"\"\"\n",
    "    missing_percent = df.isnull().mean()\n",
    "    cols_to_drop = missing_percent[missing_percent > threshold].index\n",
    "\n",
    "    print(\"üîç Dropping columns with > {:.0%} missing values:\\n\".format(threshold))\n",
    "    for col in cols_to_drop:\n",
    "        print(f\"‚ùå Dropped '{col}' ({missing_percent[col]*100:.2f}% missing)\")\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"\n",
    "    Fills missing values for training data and stores the fill values\n",
    "    so they can be reused for test data.\n",
    "    Calculates mean/mode for all columns (even those without NaNs).\n",
    "    \n",
    "    Returns:\n",
    "        df_filled: DataFrame with missing values filled\n",
    "        fill_values: dict with {column_name: fill_value}\n",
    "    \"\"\"\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'O']\n",
    "    numerical_cols = [col for col in df.columns if df[col].dtype != 'O' and col != 'SalePrice']\n",
    "\n",
    "    fill_values = {}\n",
    "    df_filled = df.copy()\n",
    "\n",
    "    print(\"\\nüîß Filling Missing Values (Training Data):\")\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        mean_val = df[col].mean()\n",
    "        df_filled[col].fillna(mean_val, inplace=True)\n",
    "        fill_values[col] = mean_val\n",
    "        print(f\"üî¢ Filled NaNs in numerical '{col}' with mean: {mean_val:.2f}\")\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        mode_val = df[col].mode()[0] if not df[col].mode().empty else None\n",
    "        df_filled[col].fillna(mode_val, inplace=True)\n",
    "        fill_values[col] = mode_val\n",
    "        print(f\"üî§ Filled NaNs in categorical '{col}' with mode: '{mode_val}'\")\n",
    "\n",
    "    return df_filled, fill_values\n",
    "\n",
    "\n",
    "def apply_fill_values(df, fill_values):\n",
    "    \"\"\"\n",
    "    Applies stored fill values to another dataset (e.g., test set).\n",
    "    \"\"\"\n",
    "    df_filled = df.copy()\n",
    "    print(\"\\nüîß Applying Stored Fill Values:\")\n",
    "    for col, val in fill_values.items():\n",
    "        df_filled[col].fillna(val, inplace=True)\n",
    "        print(f\"‚úÖ Filled NaNs in '{col}' with stored value: {val}\")\n",
    "    return df_filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = drop_high_nan_columns(train)\n",
    "\n",
    "train ,fill_values = fill_missing_values(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['Alley','MasVnrType','PoolQC','Fence','MiscFeature'])\n",
    "test = apply_fill_values(test, fill_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>3.4 Feature Engineering</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Features: We calculate the age of the house and its remodeling by subtracting the year built/remodel from the year sold, providing more meaningful temporal information.\n",
    "\n",
    "Total Bathrooms & Square Footage: We combine various bathroom and square footage features into single, more representative features like TotalBaths and TotalSF, reducing dimensionality and providing a clearer picture of the house's size.\n",
    "\n",
    "Log Transformation: Key numerical features like LotArea and SalePrice are log-transformed to reduce skewness and stabilize variance, which often improves the performance of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "\n",
    "    df['YearRemodAdd'] = [ys if yb > ys else yb for yb, ys in zip(df['YearRemodAdd'], df['YrSold'])]\n",
    "    df['YearBuilt'] = [ys if yb > ys else yb for yb, ys in zip(df['YearBuilt'], df['YrSold'])]\n",
    "    # Age features\n",
    "    for feature in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n",
    "        df[feature] = df['YrSold'] - df[feature]\n",
    "\n",
    "        # Total number of bathrooms\n",
    "    df[\"TotalBaths\"] = (df[\"FullBath\"] + 0.5 * df[\"HalfBath\"] +\n",
    "                        df[\"BsmtFullBath\"] + 0.5 * df[\"BsmtHalfBath\"])\n",
    "    df.drop(columns=[\"FullBath\", \"HalfBath\", \"BsmtFullBath\", \"BsmtHalfBath\"], inplace=True, errors='ignore')\n",
    "\n",
    "    # Total square footage\n",
    "    df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"GrLivArea\"]\n",
    "    df.drop(columns=[\"TotalBsmtSF\", \"GrLivArea\"], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Log-transform selected numeric features\n",
    "    num_features = ['LotFrontage', 'LotArea', '1stFlrSF', 'TotalSF']\n",
    "    for feature in num_features:\n",
    "        df[feature] = np.log1p(df[feature])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>3.5 Outlier Detection and Removal</H3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify and remove them from the dataset by using the Interquartile Range (IQR) method. This process is particularly focused on the features that have the highest correlation with the target variable, SalePrice, to ensure the most influential outliers are addressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Detect outliers function\n",
    "def detect_outliers(df, numerical_cols, threshold=1.5):\n",
    "    outlier_dict = {}\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "        outlier_dict[col] = list(outliers)\n",
    "    return dict(sorted(outlier_dict.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "\n",
    "# Step 2: Use the 10 most correlated features with SalePrice\n",
    "correlations = train.corr(numeric_only=True)['SalePrice'].abs()\n",
    "top10_corr_features = correlations.drop('SalePrice').sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "# Step 3: Detect outliers\n",
    "outliers = detect_outliers(train, top10_corr_features,threshold=2)\n",
    "\n",
    "# Step 4: Plot histograms in 1x5 layout\n",
    "fig, axes = plt.subplots(2, 5, figsize=(24, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top10_corr_features):\n",
    "    sns.histplot(train[feature], kde=True, ax=axes[i], bins=30)\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Top 10 Correlated Features with SalePrice (Histogram)', fontsize=16, y=1.05)\n",
    "plt.show()\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train[train.GrLivArea > 4000]['GrLivArea'].sort_values())\n",
    "# print(train[train.TotRmsAbvGrd > 14]['TotRmsAbvGrd'].sort_values())\n",
    "# print(train[train.TotalBsmtSF>4000]['TotalBsmtSF'].sort_values())\n",
    "# print(train[train['1stFlrSF']>3000]['1stFlrSF'].sort_values())\n",
    "# print(train[train['GarageArea']>1300]['GarageArea'].sort_values())\n",
    "# print(train[train['SalePrice']>500000]['SalePrice'].sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_indexes = [1182, 1298, 1169, 224]\n",
    "train = train.drop(index=rows_indexes).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "train['SalePrice'] = np.log1p(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([train,test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['YrSold'] = final_df['YrSold'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>3.6 Categorical Feature Encoding</H3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rare Label Encoding: This technique groups infrequent categories into a single \"Rare\" category, which helps to prevent overfitting to categories with very few instances.\n",
    "\n",
    "Ordinal Encoding: This method assigns a numerical value to each category based on its relationship with the target variable, SalePrice. This ordered mapping helps the model to capture the inherent hierarchy or ranking within the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rare label encoding\n",
    "categorical_cols = [col for col in final_df.columns if final_df[col].dtype == 'O']\n",
    "\n",
    "rare_encoder = RareLabelEncoder(tol=0.01, n_categories=1, replace_with='Rare', variables=categorical_cols)\n",
    "final_df = rare_encoder.fit_transform(final_df)\n",
    "\n",
    "train = final_df.iloc[:1456,:]\n",
    "test = final_df.iloc[1456:,:]\n",
    "\n",
    "# Ordinal encoding \n",
    "ordinal_encoder = OrdinalEncoder(encoding_method='ordered', variables=categorical_cols)\n",
    "train = ordinal_encoder.fit_transform(train, train['SalePrice'])\n",
    "test = ordinal_encoder.transform(test)\n",
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute full correlation matrix\n",
    "corr_matrix = train.corr(numeric_only=True)\n",
    "\n",
    "# Step 2: Create boolean mask for values > 0.7 (excluding diagonal)\n",
    "mask = (abs(corr_matrix) > 0.8) & (corr_matrix != 1.0)\n",
    "\n",
    "# Step 3: Get column names where any correlation is > 0.7\n",
    "high_corr_features = mask.any(axis=0)\n",
    "selected_features = corr_matrix.columns[high_corr_features]\n",
    "\n",
    "\n",
    "# Step 5: Plot heatmap\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(train[selected_features].corr(), annot=True, cmap='coolwarm',cbar=False)\n",
    "plt.title(\"Correlation > 0.7 Between Any Feature Pairs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>3.8 Feature Selection and Multicollinearity</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when features are highly correlated with each other, which can lead to unstable model predictions. We calculate the correlation matrix and visualize it using a heatmap to identify and remove features with a high correlation coefficient (e.g., greater than 0.8). This step ensures that our model uses a diverse set of independent variables, improving its interpretability and stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotRmsAbvGrd , Exterior2nd\n",
    "train = train.drop(columns=['GarageCars','TotRmsAbvGrd','Exterior2nd'], errors='ignore')\n",
    "test = test.drop(columns=['GarageCars','TotRmsAbvGrd','Exterior2nd'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['SalePrice'],inplace=True)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>4 Model Training and Evaluation</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>4.1 Hyperparameter Tuning with Optuna</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve the best possible performance, we use Optuna, an automatic hyperparameter optimization framework. Optuna efficiently searches for the optimal combination of hyperparameters by intelligently exploring the parameter space. We define an objective function that calculates the Root Mean Squared Error (RMSE) for each trial, and Optuna minimizes this value to find the best settings for each model. This automated process saves significant time and effort compared to manual tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Optuna for catboost\n",
    "def objective_cat(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    catboost_params = {\n",
    "        'iterations': trial.suggest_int('iterations', 1000, 8000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.08),\n",
    "        'depth': trial.suggest_int('depth', 3, 7),\n",
    "        'eval_metric': 'RMSE',\n",
    "    }\n",
    "\n",
    "    # Initialize models with suggested parameters\n",
    "    catboost_model = CatBoostRegressor(**catboost_params, verbose=0)\n",
    "    \n",
    "    # Train models\n",
    "    catboost_model.fit(train, train['SalePrice'])\n",
    "\n",
    "    # Calculate RMSE\n",
    "    kf = KFold(n_splits=10)\n",
    "    catboost_rmse = np.exp(np.sqrt(-cross_val_score(catboost_model, train, train['SalePrice'], scoring='neg_mean_squared_error', cv=kf)))\n",
    "\n",
    "    # Return average RMSE\n",
    "    return np.mean(catboost_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Optuna for xgboost\n",
    "def objective_xgb(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    xgboost_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 8000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.2, 0.6),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.4, 0.8),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 5),\n",
    "    }\n",
    "\n",
    "    # Initialize models with suggested parameters\n",
    "    xgb_model = XGBRegressor(**xgboost_params, verbosity=0)\n",
    "\n",
    "    # Train models\n",
    "    xgb_model.fit(train, train['SalePrice'])\n",
    "\n",
    "    # Calculate RMSE\n",
    "    kf = KFold(n_splits=10)\n",
    "    xgb_rmse = np.exp(np.sqrt(-cross_val_score(xgb_model, train, train['SalePrice'], scoring='neg_mean_squared_error', cv=kf)))\n",
    "\n",
    "    # Return average RMSE\n",
    "    return np.mean(xgb_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters catboost\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters xgboost\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>4.2 Model Training and Prediction</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimal hyperparameters are found, we train the XGBoost and CatBoost models using the entire training dataset. Each model is encapsulated within a Pipeline to ensure that data scaling (using MinMaxScaler) is applied consistently before training and prediction. After training, the models are used to make predictions on the validation set to evaluate their performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# ========================\n",
    "# Step 1: Prepare Data\n",
    "# ========================\n",
    "X = train.drop(columns=['Id', 'SalePrice'])\n",
    "y = train['SalePrice']\n",
    "X_test_final = test.drop(columns=['Id'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# Step 2: Use Best Parameters from Optuna\n",
    "# ========================\n",
    "best_params_xgb = study_xgb.best_params\n",
    "best_params_cat = study_cat.best_params\n",
    "\n",
    "# Ensure random_state is fixed for reproducibility\n",
    "best_params_xgb['random_state'] = 42\n",
    "best_params_cat['random_state'] = 42\n",
    "\n",
    "# ========================\n",
    "# Step 3: XGBoost Pipeline\n",
    "# ========================\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('model', XGBRegressor(**best_params_xgb))\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ========================\n",
    "# Step 4: CatBoost Pipeline\n",
    "# ========================\n",
    "cat_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),  # optional for CatBoost, but keeps consistency\n",
    "    ('model', CatBoostRegressor(**best_params_cat, verbose=0))\n",
    "])\n",
    "\n",
    "cat_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ========================\n",
    "# Step 5: Predict & Evaluate (Example for XGB)\n",
    "# ========================\n",
    "y_val_pred_xgb = xgb_pipeline.predict(X_val)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_val_pred_xgb))\n",
    "r2_xgb = r2_score(y_val, y_val_pred_xgb)\n",
    "\n",
    "print(\"XGBoost RMSE:\", rmse_xgb)\n",
    "print(\"XGBoost R¬≤:\", r2_xgb)\n",
    "\n",
    "\n",
    "y_val_pred_cat = cat_pipeline.predict(X_val)\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_val, y_val_pred_cat))\n",
    "r2_cat = r2_score(y_val, y_val_pred_cat)\n",
    "\n",
    "print(\"catBoost RMSE:\", rmse_cat)\n",
    "print(\"catBoost R¬≤:\", r2_cat)\n",
    "\n",
    "# ========================\n",
    "# Step 6: Predict on Test\n",
    "# ========================\n",
    "y_test_pred_xgb = xgb_pipeline.predict(X_test_final)\n",
    "y_test_pred_cat = cat_pipeline.predict(X_test_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>4.3 Performance Metrics</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models' performance is measured using the Root Mean Squared Error (RMSE) and R-squared (R \n",
    "2\n",
    " ) score.\n",
    "\n",
    "RMSE gives a measure of the average magnitude of the errors in the predictions. A lower RMSE indicates a more accurate model.\n",
    "\n",
    "R \n",
    "2\n",
    "  represents the proportion of the variance in the dependent variable that is predictable from the independent variables. An R \n",
    "2\n",
    "  of 1 indicates a perfect fit, while a value closer to 0 suggests the model does not explain the variability well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_xgb = np.exp(y_test_pred_xgb)\n",
    "y_test_pred_cat = np.exp(y_test_pred_cat)\n",
    "\n",
    "ypred_xgb = pd.DataFrame(y_test_pred_xgb)\n",
    "ypred_cat = pd.DataFrame(y_test_pred_cat)\n",
    "result_xgb = pd.concat([sample['Id'],ypred_xgb],axis=1)\n",
    "result_cat = pd.concat([sample['Id'],ypred_cat],axis=1)\n",
    "result_xgb.columns = ['Id', 'SalePrice']\n",
    "result_cat.columns = ['Id', 'SalePrice']\n",
    "result_xgb.to_csv('Downloads/results_xgb.csv',index=False)\n",
    "result_cat.to_csv('Downloads/results_cat.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
